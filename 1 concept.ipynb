{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a learner exposed to many AI concepts like LLMs, transformers, RAG, and more, you might feel confused about what these terms mean and how they differ. This page aims to highlight and clarify these concepts for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs  are advanced NLP models trained on vast amounts of text data, capable of generating and understanding human language<br>\n",
    "LLMs are a specific type of generative AI focused on natural language processing (NLP).\n",
    "Examples:\n",
    "- OpenAI's GPT Series (GPT-3, GPT-4,GPT-4o) \n",
    "- Google's  Gemini, Gemma ,BERT, T5 \n",
    "- Meta's  Llama 3, RoBERTa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers paper->GPT->Bert>GPT2->T5->GPT3->...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "pretraining-> fune tuning :Less data is required for fine-tuning because of the knowledge and representations that the model has gained during pre-training.\n",
    "\n",
    "encoder\n",
    "decoder\n",
    "\n",
    "- encoder-decoder model: e.g language translation \n",
    "- encoder-only model: understand the input e.g Bert\n",
    "- decoder-only model:  generative tasks ,e.g GPT \n",
    "\n",
    "self-Attention: incorporates the embeddings for all the other words in the sentence. So when processing a word  , self-attention will take a weighted average of the embeddings of the other context words.\n",
    "\n",
    "Multi-head attention: \n",
    "\n",
    "GPT:  \n",
    "- Generative : predicts a future token given the past tokens\n",
    "- Pre-trianed: trained on a large corpus of data\n",
    "- Transformer: decoder portion of the transformer architecture \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one word is about 1.4 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face\n",
    "[Hugging Face](https://huggingface.co/) provides the foundational models and data resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain\n",
    "[LangChain](https://www.langchain.com/) focuses on building applications using large language models (LLMs). It provides tools and frameworks to make it easier to develop applications that use LLMs effectivel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation (RAG)\n",
    "  \n",
    "Large Language Models (LLMs) are trained on vast volumes of data. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output, so it remains relevant, accurate, and useful in various contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector in programming \n",
    "-   a one-dimensional data strucuter \n",
    "-   homogeneous(all elements has hte same type)\n",
    "-   defined position for reach element\n",
    "-   the storage and accesss id different from lists or arrays \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization in NLP\n",
    "Vectorization is the process of converting text data into numeric values. \n",
    "- Text data need to be first transformed to equivalent numeric representations before they can be used for training or inference (Transformer models,machine learning algorithms)\n",
    "- the <B>original meaning, context, and position information</B> for the text data need to be properly represented in their numeric representations. \n",
    "- Text data is represented as a series of numeric vectors, and these vectors capture the structure and semantics of the original text. \n",
    "- Vector outputs of the models also need to be converted to their text representations using the reverse process. \n",
    "\n",
    "#### vectorization techniques\n",
    "\n",
    "- Bag of words   \n",
    "    - For each sentence, it counts the number of occurrences of each word and then forms a vector with these values. \n",
    "    - It does not capture positional information or other semantics. \n",
    "- TF-IDF(Text frequency-inverse document frequency) \n",
    "    - It creates a matrix that can be used to measure the similarity between documents. \n",
    "    - TF-IDF can create sparse matrices of documents based on the number of unique tokens encountered.\n",
    "- word embeddings \n",
    "    - also use a dictionary like bag of words, but each token in the dictionary is associated with an embedding vector. \n",
    "    - The embedding vector captures the semantic information between tokens, which helps in relating tokens of a similar meaning. \n",
    "- Sentence embeddings \n",
    "    - a single embedding vector is created for an entire sentence or paragraph using machine learning models. \n",
    "    - It is popular for processing inputs and outputs in large language model-based applications. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector similarity search\n",
    " \n",
    "  Each vector is a series of data points represented in n dimensions. <br>\n",
    "  A given sentence or paragraph can be represented as a vector using sentence embeddings. <br>\n",
    "  Similarity measures are used to find how close two given vectors are(How the two sentences are similar in their semantics.)\n",
    "\n",
    "  <b>Dstiance measures  </b>\n",
    "   - L2:Euclidean distance  \n",
    "   - IP:linner product or IP\n",
    "   - cosine similarity.\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
