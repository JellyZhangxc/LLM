{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a learner exposed to many AI concepts like LLMs, transformers, RAG, and more, you might feel confused about what these terms mean and how they differ. This page aims to highlight and clarify these concepts for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs  are advanced NLP models trained on vast amounts of text data, capable of generating and understanding human language<br>\n",
    "LLMs are a specific type of generative AI focused on natural language processing (NLP).\n",
    "Examples:\n",
    "- OpenAI's GPT Series (GPT-3, GPT-4,GPT-4o) \n",
    "- Google's  Gemini, Gemma ,BERT, T5 \n",
    "- Meta's  Llama 3, RoBERTa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers paper->GPT->Bert>GPT2->T5->GPT3->...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "pretraining-> fune tuning :Less data is required for fine-tuning because of the knowledge and representations that the model has gained during pre-training.\n",
    "\n",
    "encoder\n",
    "decoder\n",
    "\n",
    "- encoder-decoder model: e.g language translation \n",
    "- encoder-only model: understand the input e.g Bert\n",
    "- decoder-only model:  generative tasks ,e.g GPT \n",
    "\n",
    "self-Attention: incorporates the embeddings for all the other words in the sentence. So when processing a word  , self-attention will take a weighted average of the embeddings of the other context words.\n",
    "\n",
    "Multi-head attention: \n",
    "\n",
    "GPT:  \n",
    "- Generative : predicts a future token given the past tokens\n",
    "- Pre-trianed: trained on a large corpus of data\n",
    "- Transformer: decoder portion of the transformer architecture \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one word is about 1.4 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face\n",
    "[Hugging Face](https://huggingface.co/) provides the foundational models and data resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain\n",
    "[LangChain](https://www.langchain.com/) focuses on building applications using large language models (LLMs). It provides tools and frameworks to make it easier to develop applications that use LLMs effectivel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation (RAG)\n",
    "  \n",
    "Large Language Models (LLMs) are trained on vast volumes of data. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output, so it remains relevant, accurate, and useful in various contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector in programming \n",
    "-   a one-dimensional data strucuter \n",
    "-   homogeneous(all elements has hte same type)\n",
    "-   defined position for reach element\n",
    "-   the storage and accesss id different from lists or arrays \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
