{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b94809",
   "metadata": {},
   "source": [
    "### 1 Create the Connection and database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c07a8764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases:  ['default', 'cache_db']\n",
      "Creating database : rag_db\n"
     ]
    }
   ],
   "source": [
    "#Create the Connection and database for RAG\n",
    "from pymilvus import connections\n",
    "from pymilvus import db,Collection\n",
    "\n",
    "from pymilvus import utility\n",
    "\n",
    "connections.add_connection(\n",
    "    rag_conn={\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": \"19530\",\n",
    "        \"username\" : \"username\",\n",
    "        \"password\" : \"password\"\n",
    "    })\n",
    "\n",
    "conn_name=\"rag_conn\"\n",
    "db_name=\"rag_db\"\n",
    "\n",
    "connections.connect(conn_name)\n",
    "connections.list_connections()\n",
    "\n",
    "current_dbs=db.list_database(using=conn_name)\n",
    "print(\"Databases: \", current_dbs)\n",
    "\n",
    "if ( db_name not in current_dbs):\n",
    "    print(\"Creating database :\", db_name)\n",
    "    resume_db = db.create_database(db_name, using=conn_name) \n",
    "\n",
    "#Switch to the new database\n",
    "db.using_database(db_name, using=conn_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eb80c1",
   "metadata": {},
   "source": [
    "### 2 Create a new collection for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee43eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections:  ['rag_collection']\n",
      "\n",
      " Schema : {'auto_id': False, 'description': 'RAG Schema', 'fields': [{'name': 'chunk_id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': False}, {'name': 'rag_text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 2048}}, {'name': 'rag_embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 1536}}], 'enable_dynamic_field': True}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pymilvus import CollectionSchema, FieldSchema, DataType, Collection\n",
    "import json\n",
    "\n",
    "chunk_id_field = FieldSchema(\n",
    "    name=\"chunk_id\",\n",
    "    dtype=DataType.INT64,\n",
    "    is_primary=True,\n",
    "    max_length=32)\n",
    "\n",
    "rag_text_field= FieldSchema(\n",
    "    name=\"rag_text\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=2048)\n",
    "\n",
    "rag_embedding_field = FieldSchema(\n",
    "    name=\"rag_embedding\",\n",
    "    dtype=DataType.FLOAT_VECTOR,\n",
    "    dim=1536 #Define based on embedding used\n",
    ")\n",
    "\n",
    "rag_schema=CollectionSchema(\n",
    "    fields=[chunk_id_field, rag_text_field, rag_embedding_field],\n",
    "    description=\"RAG Schema\",\n",
    "    enable_dynamic_field=True\n",
    ")\n",
    "\n",
    "collection_name=\"rag_collection\"\n",
    "\n",
    "rag_collection=Collection(\n",
    "    name=collection_name,\n",
    "    schema=rag_schema,\n",
    "    using=conn_name,\n",
    "    shard_num=2\n",
    ")\n",
    "\n",
    "from pymilvus import utility\n",
    "print(\"Collections: \", utility.list_collections(using=conn_name))\n",
    "\n",
    "r_collection=Collection(collection_name, using=conn_name)\n",
    "print(\"\\n Schema :\", r_collection.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb7109",
   "metadata": {},
   "source": [
    "### 3 Preparing data for Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3c41079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upgrade vesions otherwiese will meet the error \"No module named  pwd\"  when from langchain.document_loaders\n",
    "#https://github.com/langchain-ai/langchain/issues/17514\n",
    "\n",
    "# !pip install langchain==0.1.6\n",
    "# !pip install langchain-community==0.0.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9348040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load up the PDF document\n",
    "from langchain.document_loaders import PDFMinerLoader\n",
    "\n",
    "loader = PDFMinerLoader(\"Large Language Models.pdf\")\n",
    "pdf_docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2dad2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks : 23\n",
      "Sample chunk text:  As autoregressive language models, they work by taking an input text and repeatedly predicting the \n",
      "next token or word. Up to 2020, fine tuning was the only way a model could be adapted to be able to \n",
      "accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to \n",
      "achieve similar results. They are thought to acquire knowledge about syntax, semantics and \n",
      "\"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the\n"
     ]
    }
   ],
   "source": [
    "#Split document into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter   =   RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512, # Specify the character chunk size\n",
    "    chunk_overlap=32, # \"Allowed\" Overlap across chunks\n",
    "    length_function=len # Function used to evaluate the chunk size (here in terms of characters)\n",
    ")\n",
    "\n",
    "pdf_docs    =   text_splitter.split_documents(pdf_docs)\n",
    "\n",
    "#Create a list of chunks\n",
    "rag_text =[]\n",
    "for i in pdf_docs:\n",
    "    rag_text.append(i.page_content)\n",
    "    \n",
    "print(\"Total chunks :\", len(rag_text))\n",
    "print(\"Sample chunk text: \", rag_text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d79ea",
   "metadata": {},
   "source": [
    "### 4 embedding chrucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84373100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  The return value of the function being wrapped.\n"
     ]
    }
   ],
   "source": [
    "#create embeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY=\" \"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "rag_embedding=[embeddings_model.embed_query(i) \n",
    "                  for i in rag_text]\n",
    "\n",
    "#Create chunk IDs \n",
    "record_ids=[i for i in range(len(rag_text))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f0bf8",
   "metadata": {},
   "source": [
    "### 5 Populating the Milvus database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e15e054a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_rows': 23, 'indexed_rows': 23, 'pending_index_rows': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare data columns by columns\n",
    "insert_data=[record_ids, rag_text, rag_embedding]\n",
    "\n",
    "#i_collection = Collection(collection_name, using=conn_name)\n",
    "\n",
    "#Insert the records\n",
    "mr=r_collection.insert(insert_data)\n",
    "#Flush the inserted records\n",
    "r_collection.flush()\n",
    "\n",
    "#Build an index on the embedding field\n",
    "index_params = {\n",
    "    \"metric_type\":\"L2\",\n",
    "    \"index_type\":\"IVF_FLAT\",\n",
    "    \"params\" :{\"nlist\":1024}\n",
    "}\n",
    "\n",
    "r_collection.create_index(\n",
    "    field_name=\"rag_embedding\",\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "utility.index_building_progress(collection_name, using=conn_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620eb45a",
   "metadata": {},
   "source": [
    "### 6 Answering questions - similarity search from knowlage base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "268f107b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top result : id: 8, distance: 0.20659318566322327, entity: {'rag_text': 'Gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced \\ntowards one gender over another. This bias typically arises from the data on which these models are \\ntrained. For example, large language models often assign roles and characteristics based on \\ntraditional gender norms; it might associate nurses or secretaries predominantly with women and \\nengineers or CEOs with men.'}\n"
     ]
    }
   ],
   "source": [
    "#The retrieval process\n",
    "search_params = {\n",
    "    \"metric_type\": \"L2\", \n",
    "    \"offset\": 0, \n",
    "    \"ignore_growing\": False, \n",
    "    \"params\": {\"nprobe\": 20, \"radius\":0.5}\n",
    "}\n",
    "\n",
    "query = \"What is gender bias?\"\n",
    "search_embed=embeddings_model.embed_query(query)\n",
    "#print(search_embed)\n",
    "\n",
    "q_collection = Collection(collection_name, using=conn_name)\n",
    "\n",
    "#load into memory before search\n",
    "q_collection.load()\n",
    "\n",
    "results=q_collection.search(\n",
    "    data=[search_embed],\n",
    "    anns_field=\"rag_embedding\",\n",
    "    param=search_params,\n",
    "    limit=3, #Get top 3 results only\n",
    "    expr=None,\n",
    "    output_fields=[\"rag_text\"],\n",
    "    consistency_level=\"Strong\"\n",
    ")\n",
    "\n",
    "print(\"Top result :\", results[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "568b54d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top result : id: 6, distance: 0.35395103693008423, entity: {'rag_text': 'Language bias refers a type of statistical sampling bias tied to the language of a query that leads to \\n\"a systematic deviation in sampling information that prevents it from accurately representing the \\ntrue coverage of topics and views available in their repository.\" Luo et al. show that current large \\nlanguage models, as they are predominately trained on English-language data, often present the \\nAnglo-American views as truth, while systematically downplaying non-English perspectives as'}\n",
      "Top result : id: 9, distance: 0.3606182932853699, entity: {'rag_text': 'Beyond gender and race, these models can reinforce a wide range of stereotypes, including those \\nbased on age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or \\ncaricature groups of people, sometimes in harmful or derogatory ways.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Top result :\", results[0][1])\n",
    "print(\"Top result :\", results[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94039d",
   "metadata": {},
   "source": [
    "### 7 Generate prompt with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ece4840b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on only the context provided, answer the query below:  Context: ['Gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced \\ntowards one gender over another. This bias typically arises from the data on which these models are \\ntrained. For example, large language models often assign roles and characteristics based on \\ntraditional gender norms; it might associate nurses or secretaries predominantly with women and \\nengineers or CEOs with men.', 'Language bias refers a type of statistical sampling bias tied to the language of a query that leads to \\n\"a systematic deviation in sampling information that prevents it from accurately representing the \\ntrue coverage of topics and views available in their repository.\" Luo et al. show that current large \\nlanguage models, as they are predominately trained on English-language data, often present the \\nAnglo-American views as truth, while systematically downplaying non-English perspectives as', 'Beyond gender and race, these models can reinforce a wide range of stereotypes, including those \\nbased on age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or \\ncaricature groups of people, sometimes in harmful or derogatory ways.']\n",
      "\n",
      " Query: What is gender bias?\n"
     ]
    }
   ],
   "source": [
    "#Prepare prompt for LLM\n",
    "\n",
    "context=[]\n",
    "\n",
    "#Append all returned chunks\n",
    "for i in range(len(results[0])):\n",
    "    context.append(results[0][i].entity.get(\"rag_text\"))\n",
    "\n",
    "#Create a prompt\n",
    "prompt= (\"Based on only the context provided, answer the query below: \"\n",
    "        + \" Context: \" + str(context)\n",
    "        + \"\\n\\n Query: \" + query)\n",
    "        \n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb46b8c7",
   "metadata": {},
   "source": [
    "### 8 Use LLM to generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fc5f933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  The return value of the function being wrapped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender bias refers to the tendency of models to produce outputs that are unfairly prejudiced towards one gender over another, often arising from the data on which the models are trained. This bias can lead to assigning roles and characteristics based on traditional gender norms, such as associating certain professions predominantly with one gender.\n"
     ]
    }
   ],
   "source": [
    "#Generate with LLM\n",
    "\n",
    "# from langchain.llms import OpenAI\n",
    "\n",
    "# llm= OpenAI(temperature=0., model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# completion=llm(prompt)\n",
    "# print(completion)\n",
    "\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\", temperature=0)\n",
    "messages = [\n",
    "            (\"human\", f\"{prompt}\"),\n",
    "        ]\n",
    "llm_response= llm.invoke(messages).content\n",
    "print(llm_response)\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
