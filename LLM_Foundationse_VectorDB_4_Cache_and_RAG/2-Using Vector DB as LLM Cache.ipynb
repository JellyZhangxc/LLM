{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e5dcda",
   "metadata": {},
   "source": [
    "### 1. Connecting to Milvus and create database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "492aa787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_db : Database already exists\n"
     ]
    }
   ],
   "source": [
    "#Setup database & collection\n",
    "from pymilvus import connections\n",
    "from pymilvus import db,Collection\n",
    "\n",
    "from pymilvus import utility\n",
    "\n",
    "#Names for connections, database and collections\n",
    "conn_name = \"cache_conn\"\n",
    "db_name=\"cache_db\"\n",
    "collection_name=\"llm_cache\"\n",
    "\n",
    "#Create a connection to Milvus\n",
    "connections.add_connection(\n",
    "    cache_conn={\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": \"19530\",\n",
    "        \"username\" : \"username\",\n",
    "        \"password\" : \"password\"\n",
    "    })\n",
    "\n",
    "\n",
    "#Connect\n",
    "connections.connect(conn_name)\n",
    "\n",
    "#Create a DB if not already present\n",
    "current_dbs=db.list_database(using=conn_name)\n",
    "\n",
    "if ( db_name not in current_dbs):\n",
    "    print(\"Creating database :\", db_name)\n",
    "    resume_db = db.create_database(db_name, using=conn_name) #default db is \"default\"\n",
    "else:\n",
    "    print(db_name, \": Database already exists\")\n",
    "\n",
    "#Switch to the new database\n",
    "db.using_database(db_name, using=conn_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807afe17",
   "metadata": {},
   "source": [
    "### 2. Creating collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d1aece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema :  {'auto_id': True, 'description': 'Cache for LLM', 'fields': [{'name': 'cache_id', 'description': '', 'type': <DataType.INT64: 5>, 'is_primary': True, 'auto_id': True}, {'name': 'prompt_text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 2048}}, {'name': 'response_text', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 2048}}, {'name': 'prompt_embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 1536}}], 'enable_dynamic_field': True} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a Collection for cache\n",
    "from pymilvus import CollectionSchema, FieldSchema, DataType, Collection\n",
    "import json\n",
    "\n",
    "#Define fields in the cache\n",
    "#Autogenerated ID field for each entity\n",
    "cache_id = FieldSchema(\n",
    "    name=\"cache_id\",\n",
    "    dtype=DataType.INT64,\n",
    "    auto_id=True,\n",
    "    is_primary=True,\n",
    "    max_length=32)\n",
    "\n",
    "#Text for the input prompt\n",
    "prompt_text= FieldSchema(\n",
    "    name=\"prompt_text\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=2048)\n",
    "\n",
    "#Text for the LLM response\n",
    "response_text= FieldSchema(\n",
    "    name=\"response_text\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=2048)\n",
    "\n",
    "#Embedding for the input prompt\n",
    "prompt_embedding = FieldSchema(\n",
    "    name=\"prompt_embedding\",\n",
    "    dtype=DataType.FLOAT_VECTOR,\n",
    "    dim=1536 #Define based on embedding used\n",
    ")\n",
    "\n",
    "#Define the schema for the cache collection\n",
    "cache_schema=CollectionSchema(\n",
    "    fields=[cache_id, prompt_text, response_text, prompt_embedding],\n",
    "    description=\"Cache for LLM\",\n",
    "    enable_dynamic_field=True\n",
    ")\n",
    "\n",
    "#Create the collection\n",
    "cache_collection=Collection(\n",
    "    name=collection_name,\n",
    "    schema=cache_schema,\n",
    "    using=conn_name,\n",
    "    shard_num=2\n",
    ")\n",
    "\n",
    "print(\"Schema : \", cache_collection.schema, \"\\n\")\n",
    "\n",
    "#Build an index for the prompt embedding field\n",
    "index_params = {\n",
    "    \"metric_type\":\"L2\",\n",
    "    \"index_type\":\"IVF_FLAT\",\n",
    "    \"params\" :{\"nlist\":1024}\n",
    "}\n",
    "\n",
    "cache_collection.create_index(\n",
    "    field_name=\"prompt_embedding\",\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "#Flush the collection to persist\n",
    "cache_collection.flush()\n",
    "#Load the collection in memory\n",
    "cache_collection.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c8a48",
   "metadata": {},
   "source": [
    "##  3 save the [prompt, response, prompt embedding] into Milvus as cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27c32ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "import time\n",
    "\n",
    " \n",
    "\n",
    "#If you use the free tier, you may hit rate limits with the number of requests\n",
    "\n",
    "OPENAI_API_KEY=\" \"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "#Create an LLM object\n",
    "#llm= OpenAI(temperature=0., model=\"text-davinci-003\")  #The model `text-davinci-003` has been deprecated\n",
    "#llm= OpenAI(temperature=0., model=\"gpt-3.5-turbo\")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "#Setup embedding model for creating embeddings\n",
    "# embeddings_model = OpenAIEmbeddings()\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model=\"text-embedding-ada-002\")\n",
    "#setup threshold for similarity between vectors\n",
    "similarity_threshold=0.3\n",
    "\n",
    "search_params = {\n",
    "    \"metric_type\": \"L2\", \n",
    "    \"offset\": 0, \n",
    "    \"ignore_growing\": False, \n",
    "    \"params\": {\"nprobe\": 20, \"radius\":similarity_threshold}\n",
    "}\n",
    "\n",
    "#create a function to run the inference loop\n",
    "def get_response(question):\n",
    "    \n",
    "    start_time=time.time()\n",
    "    # step 1  create embedding for incoming prompt\n",
    "    prompt_embed=embeddings_model.embed_query(question)\n",
    "    \n",
    "    # step 2 Check cache if result exists\n",
    "    cache_results=cache_collection.search(\n",
    "        data=[prompt_embed],  #embedding of the input query to search for\n",
    "        anns_field=\"prompt_embedding\",#field to search with ANN\n",
    "        param=search_params,  #earch_params with metric_type,\n",
    "        limit=1, #Look for the top result only\n",
    "        expr=None, # if use additional scalar conditions\n",
    "        output_fields=[\"prompt_text\", \"response_text\"],\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "        \n",
    "    returned_response =\"None\"\n",
    "    \n",
    "    if ( len(cache_results[0]) > 0 ):\n",
    "        \n",
    "        #Cache hit\n",
    "        print(question, \" :\\n Cache hit : \",cache_results[0])\n",
    "        returned_response = cache_results[0][0].entity.get(\"response_text\")\n",
    "    \n",
    "    else:\n",
    "        ## step 2-1  Find answer with ChatOpenAI\n",
    "        messages = [\n",
    "            (\"human\", f\"{question}\"),\n",
    "        ]\n",
    "        llm_response= llm.invoke(messages).content\n",
    "        \n",
    "        print(question, \":\\n LLM returned :\", llm_response)\n",
    "        returned_response = question\n",
    "        \n",
    "        #step 2-1 save prompt/response to cache\n",
    "        prompt_text = [question]\n",
    "        prompt_embedding=[prompt_embed]\n",
    "        response_text = [llm_response]\n",
    "        \n",
    "        #Format for data input\n",
    "        insert_data=[prompt_text, response_text, prompt_embedding]\n",
    "        #insert into collection(table)\n",
    "        mr=cache_collection.insert(insert_data)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"Time elapsed :\",  end_time - start_time, \"\\n\")\n",
    "    return returned_response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e003faab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In which year was Abraham Lincoln born? :\n",
      " LLM returned : Abraham Lincoln was born on February 12, 1809.\n",
      "Time elapsed : 2.2014362812042236 \n",
      "\n",
      "What is distance between the sun and the moon? :\n",
      " LLM returned : The average distance between the sun and the moon is approximately 238,855 miles (384,400 kilometers).\n",
      "Time elapsed : 1.9139692783355713 \n",
      "\n",
      "How many years have Lebron James played in the NBA? :\n",
      " LLM returned : LeBron James has played in the NBA for 19 seasons as of the 2021-2022 season.\n",
      "Time elapsed : 2.0807125568389893 \n",
      "\n",
      "What are the advantages of the python language? :\n",
      " LLM returned : 1. Easy to learn and use: Python has a simple and easy-to-read syntax, making it a great language for beginners to learn programming.\n",
      "\n",
      "2. Versatile: Python can be used for a wide range of applications, including web development, data analysis, artificial intelligence, machine learning, and more.\n",
      "\n",
      "3. Large standard library: Python comes with a large standard library that provides a wide range of modules and packages for various tasks, reducing the need to write code from scratch.\n",
      "\n",
      "4. Community support: Python has a large and active community of developers who contribute to its development, provide support, and create libraries and frameworks that extend its capabilities.\n",
      "\n",
      "5. Cross-platform compatibility: Python is a cross-platform language, meaning that code written in Python can run on different operating systems without any modifications.\n",
      "\n",
      "6. Integration capabilities: Python can easily integrate with other languages and technologies, making it a versatile choice for building complex applications.\n",
      "\n",
      "7. Scalability: Python is scalable and can be used to build small scripts as well as large-scale applications, making it suitable for projects of any size.\n",
      "\n",
      "8. Open-source: Python is an open-source language, meaning that it is free to use and distribute, and its source code is available for anyone to modify and contribute to.\n",
      "Time elapsed : 7.035189867019653 \n",
      "\n",
      "What is the typical height of an elephant :\n",
      " LLM returned : The typical height of an elephant at the shoulder is around 8 to 13 feet (2.4 to 4 meters).\n",
      "Time elapsed : 3.365983724594116 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Build up the cache\n",
    "response=get_response(\"In which year was Abraham Lincoln born?\")\n",
    "response=get_response(\"What is distance between the sun and the moon?\")\n",
    "response=get_response(\"How many years have Lebron James played in the NBA?\")\n",
    "response=get_response(\"What are the advantages of the python language?\")\n",
    "response=get_response(\"What is the typical height of an elephant\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5e0df",
   "metadata": {},
   "source": [
    "##  4  cache hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c7405f61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List some advantages of the python language  :\n",
      " Cache hit :  [\"id: 451225468120105048, distance: 0.04900672659277916, entity: {'prompt_text': 'What are the advantages of the python language?', 'response_text': '1. Easy to learn and use: Python has a simple and easy-to-read syntax, making it a great language for beginners to learn programming.\\\\n\\\\n2. Versatile: Python can be used for a wide range of applications, including web development, data analysis, artificial intelligence, machine learning, and more.\\\\n\\\\n3. Large standard library: Python comes with a large standard library that provides a wide range of modules and packages for various tasks, reducing the need to write code from scratch.\\\\n\\\\n4. Community support: Python has a large and active community of developers who contribute to its development, provide support, and create libraries and frameworks that extend its capabilities.\\\\n\\\\n5. Cross-platform compatibility: Python is a cross-platform language, meaning that code written in Python can run on different operating systems without any modifications.\\\\n\\\\n6. Integration capabilities: Python can easily integrate with other languages and technologies, making it a versatile choice for building complex applications.\\\\n\\\\n7. Scalability: Python is scalable and can be used to build small scripts as well as large-scale applications, making it suitable for projects of any size.\\\\n\\\\n8. Open-source: Python is an open-source language, meaning that it is free to use and distribute, and its source code is available for anyone to modify and contribute to.'}\"]\n",
      "Time elapsed : 1.7247002124786377 \n",
      "\n",
      "How tall is an elephant?  :\n",
      " Cache hit :  [\"id: 451225468120105050, distance: 0.11606171727180481, entity: {'prompt_text': 'What is the typical height of an elephant', 'response_text': 'The typical height of an elephant at the shoulder is around 8 to 13 feet (2.4 to 4 meters).'}\"]\n",
      "Time elapsed : 1.0262947082519531 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=get_response(\"List some advantages of the python language\")\n",
    "response=get_response(\"How tall is an elephant?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaa5ff8",
   "metadata": {},
   "source": [
    "##  5 not cache hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8456c60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How old are you? :\n",
      " LLM returned : I am an AI digital assistant, so I do not have an age in the traditional sense.\n",
      "Time elapsed : 2.1015889644622803 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=get_response(\"How old are you?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
