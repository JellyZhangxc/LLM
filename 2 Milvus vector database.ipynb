{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "The worker node is responsible for loading indexes, searching, and returning results back<br>\n",
    "The object storage is used to store  the collections and indexes.\n",
    "\n",
    " <img src=\"documents/Milvus arch.PNG\" alt=\"drawing\" style=\"width:500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Each Milvus instance can manange multiple databases, the default one is \"default\"\n",
    "2. Each database  serves as a container storeing collections, partitions, and indexes within it. \n",
    "3. Collection is like table with schema which defined fields for data storage; support saclar and vector fields; PR or auto-generated keys</br>\n",
    "   Datatypes supported</br>\n",
    "   <img src=\"documents/Milvus dtypes.PNG\" alt=\"drawing\" style=\"width:500px;\"/>\n",
    "\n",
    "3. RBAC(access control) is implemented by database. \n",
    "- Users can be created and configured at a database level. \n",
    "- Roles can also be created for each database with specified permissions and then assigned to users.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partations\n",
    "- A collection  can be split up into multiple partitions.  \n",
    "- All data in a partition are stored physically together. \n",
    "- Default partition called _default.  \n",
    "- Data can be inserted into or queried from partitions specially   \n",
    "- Partitions help optimize storage and retrieval: use popular filter fieldsas partition keys.  ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index\n",
    "Speed up searching\n",
    "- In Milvus, we can create indexes on either scalar or vector fields. \n",
    "- only be one index per field.  no support for composite indexes in Milvus. \n",
    "- Indexes help organize vectors using an ANN(apporximate nearest neighor) metric type like L2 or IP. The index is set up in such a way that will help search using these metrics efficiently. \n",
    "- Vector indexes are a <B>prerequisite</B> for doing ANN searches on vector fields. These indexes must be created before any such search. \n",
    "- Index types</BR>\n",
    "      <img src=\"documents/Milvus index type.PNG\" alt=\"drawing\" style=\"width:500px;\"/>\n",
    "    ```\n",
    "    index_params = {\n",
    "        \"metric_type\":\"L2\",\n",
    "        \"index_type\":\"IVF_FLAT\",\n",
    "        \"params\" :{\"nlist\":1024}\n",
    "    }\n",
    "    collection_vr.create_index(\n",
    "        field_name=\"the embedded field\",   #create index on   vector field\n",
    "        index_params=index_params)\n",
    "\n",
    "    utility.index_building_progress(collection_name,using=connection_id)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data management\n",
    "- Row also called entity\n",
    "- Bulk inserts are recommended \n",
    "   1. define  CollectionSchema\n",
    "   2. create Collection\n",
    "   3. prepare insert_data based on the schmema ,  each field is a list which means we generate data column by coolumn, NOT row by row<br>\n",
    "    in general, the embedding field is FLOAT_VECTOR, created by ```embeddings_model.embed_query(the content for embedding)```<br>\n",
    "    the finnal insert_data is [list1, list2,.....listembedding] \n",
    "   4. Bulk insert ```Collection.insert(insert_data)```<br>\n",
    "\n",
    "   \n",
    "- Flush operation    \n",
    "    - After inserts are done, a flush operation is needed to index the newly inserted data \n",
    "    - Milvus automatically flushes data after the pending records reach a specific size after insertion   \n",
    "    - But if immediate querying is needed, it is recommended to manually trigger the flush operation \n",
    "\n",
    "- Create index\n",
    "    ```\n",
    "    index_params = {\n",
    "        \"metric_type\":\"L2\",\n",
    "        \"index_type\":\"IVF_FLAT\",\n",
    "        \"params\" :{\"nlist\":1024}\n",
    "    }\n",
    "\n",
    "    course_collection.create_index(\n",
    "        field_name=\"desc_embedding\",   #create index on   vector field\n",
    "        index_params=index_params\n",
    "    )\n",
    "\n",
    "    utility.index_building_progress(collection_name,using=connection_id)\n",
    "    ```\n",
    "\n",
    "- Querying scalar data or search vecotr filed \n",
    "   - A collection should first be loaded into memory before queries can be executed against it<br>\n",
    "    ``` course_collection.load()```\n",
    "\n",
    "\n",
    "- Upsert operation      \n",
    "    - Milvus also supports the upsert operation   \n",
    "    - if a duplicate record is inserted with the same primary key, the existing record is updated rather than creating a new record   \n",
    "- Delete   \n",
    "    - Entity can also be deleted using the primary key or a Boolean expression as a filter.  ```course_collection.delete()```\n",
    "    - Drop a collection ```utility.drop_collection(collection_name,using=connection_id)```\n",
    "    - Drop a database ```db.drop_database(db_name, using=connection_id)```\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query \n",
    "- sql statement\n",
    "- can secifily fileds, partititons,limints and offsets(which is the number of rows to skip before returning the remaining data. This helps with pagination type querie)  \n",
    "- aggration: only count support\n",
    "- Filtering:  like  && ||   == != >= <=  in  array_contains  json_contains ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Search vector\n",
    "1. the input string or the search query should first be converted to a vector using the <b>same embedding model</b> as the one used when ingesting the vector field. \n",
    "2. The metric used for comparison should be the <b>same metric(e.g IP,L2)</b> that was used when creating the index for the vector field.\n",
    "index is a prerequisite before search can be performed on the vector field.\n",
    "3. can also specify the limit on the number of rows returned and an offset from which to return rows. \n",
    "4. can also specify radius parameter to filter based on similarity(distance). The smaller the distance, the higher the similarity. \n",
    "Do note that the range of values for distance will vary based on the metric type used, so radius needs to be adjusted for that. \n",
    "5. the computed distance is also returned in addition to the query results. \n",
    "\n",
    "\n",
    "- <b>L2:  the smaller the closer\n",
    "- cos: the larger the closer</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up  env to practise Mivus\n",
    "1. cd to the Folder A where the compose yml is <br>\n",
    "``` cd LLM_Foundationse_VectorDB_4__Cach_and_RAG\\Exercise Files```\n",
    "2. Create and start a container to install Mivus<br>\n",
    "```docker-compose -f milvus-standalone-docker-compose.yml up -d```  <br>\n",
    " <img src=\"documents/Milvus docker container.PNG\" alt=\"drawing\" style=\"width:600px;\"/>\n",
    "3. List containers to check the contianter is ready<br>\n",
    "``` docker ps (or docker container ls)```<br>\n",
    "<img src=\"documents/Milvus running containers.PNG  \" alt=\"drawing\" style=\"width:600px;\"/>\n",
    "4. strat the miwus UI<br>\n",
    "``` localhost:8000/#/connect``` <BR>\n",
    "<img src=\"documents/Milvus local management UI .PNG \" alt=\"drawing\" style=\"width:600px;\"/>\n",
    "5. create a new vitral env name Milus<br>\n",
    "```conda create --name Mivus   python=3.11.5```\n",
    "6. active the env under Folder A <br>\n",
    "```conda active Mivus ```\n",
    "\n",
    "<br>\n",
    " \n",
    "For  step 5&6 , I would like to run the notebook using VS code installed before Anaconda <br> \n",
    "when run ```conda create --name Mivus   python=3.11.5``` in powershell<br>\n",
    "I got the error <br>\n",
    " > \"The term 'conda' is not recognized as the name of a cmdlet, function, script file, or operable program.\"\n",
    "\n",
    "After \n",
    "1. install Anaconda \n",
    "2. add the system path, \n",
    "3. run ```conda init powershell```   in Anaconda PowerShell prompt, to add conda into the normal PowerShell's path<br>\n",
    "the command above still did not work in VS'PowerShell <br>\n",
    "But it works in Anaconda powershell prompt<br>\n",
    "  <img src=\"documents/conda evn .PNG \" alt=\"drawing\" style=\"width:600px;\"/>\n",
    "\n",
    "\n",
    "4. Add <b>Python extension</b> for VS Code,Python extension works with conda fine,Create a conda environment and the extension will allow you to select it as your environment/interpreter. <br>\n",
    " <img src=\"documents/change your Python interpreter  VSCode .PNG \" alt=\"drawing\" style=\"width:600px;\"/><br>\n",
    " And ```conda create --name Mivus   python=3.11.5```  works as well<br>\n",
    " we could swith from PowerShell to Command Prompt to go into the conda (Mivus) environment.<br>\n",
    " \n",
    "https://stackoverflow.com/questions/54828713/working-with-anaconda-in-visual-studio-code\n",
    " \n",
    "- conda create --name Mivus   python=3.11.5\n",
    "- conda activate Mivus\n",
    "- conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector DB used as LLM Cache\n",
    "\n",
    "Reason:\n",
    "LLM is expensive (time and cost)\n",
    "- reduce cost and lanency\n",
    "\n",
    "Cache: prompt, response, prompt embedding<br>\n",
    "\n",
    "workfolw:<br>\n",
    "  <img src=\"documents/prompt cache workflow.PNG\" alt=\"drawing\" style=\"width:600px;\"/>\n",
    "\n",
    "Best practise\n",
    "- track the cache hit rate to evelaute the cache efficecent \n",
    "- benchmark\n",
    "- limit size of cached entries:A cache can grow too big over time, impacting the efficiency and relevancy of the results. Set a limit for the cache size and manage it over time. \n",
    "- It's recommended to add a last used timestamp to the cache collection and update it every time a cached entry is returned to the user. \n",
    "This helps track which entries are often used and which ones are not.\n",
    "- To control the cache size, prune entries in the cache. It is recommended to prune them based on their age as well\n",
    "- get user feedback on if the answers returned from the cache are correct and relevant.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG:Retrieval-augmented generation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shortcomings of LLM \n",
    "- LLMs can only answer questions based on the data they are trained on.  \n",
    "- The answers from the LLMs may not be current. Their cut-off date is usually the date on which their original training data sources are extracted.\n",
    "- LLMs have a tendency to hallucinate. They sometimes provide make-believe answers that are not factually correct. \n",
    "- for enterprise use cases, LLMs cannot answer questions based on enterprise or confidential data where this data is not part of the training dataset. \n",
    "- It is possible to build custom LLMs using organizational data only, but that can prove to be expensive to build. It is also expensive to keep the LLM updated with new data on a daily basis. \n",
    "\n",
    "\n",
    "#### RAG: Knowledge curation process<br>\n",
    "  <img src=\"documents/curation.PNG\" alt=\"drawing\" style=\"width:600px;\"/><BR>\n",
    "-  For text data that needs to be converted to vectors, we need to do chunking. <BR>\n",
    "  vector field can only hold a limited amount of data. <BR>\n",
    "  Also, when a prompt is issued, we want to only retrieve a small part of the original content that contains relevant information about the prompt. <BR>\n",
    "  For this, we split up the original text into chunks of equal sizes. <BR>\n",
    "  The size of the chunks may vary based on the use case, but it's usually 1024. <BR>\n",
    "  Each chunk is stored as a separate row or entity in the vector database.<BR>\n",
    "-  Once chunks are available, we need to convert these individual chunks into embeddings using an embedding model.  ```embeddings_model.embed_query()```\n",
    "\n",
    "\n",
    "#### RAG:question-answering process\n",
    "  <img src=\"documents/QandA.PNG\" alt=\"drawing\" style=\"width:600px;\"/>\n",
    "\n",
    "\n",
    "#### Applications scenarios \n",
    "-  interactive chatbots that businesses use to communicate with their customers. \n",
    "-  RAG can help in automated responses to customer queries by email.\n",
    "-  RAG can help with root cause analysis of technical issues faced. Based on log messages, absorbed metrics, and information from manuals\n",
    "- e-commerce search:RAG can help customers quickly find what they are searching for and provide good narratives about the product or service. They can also customize such information for the customer. Enterprises have help desk for functions like human  repeating. \n",
    "- Desk help  for functions like human resources, legal, or logistics.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
