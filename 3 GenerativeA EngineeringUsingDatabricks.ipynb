{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  RAG -Retrieval-Augmented Generation  \n",
    "\n",
    "### Language Model learn knowleage\n",
    "1. Model Pre-Training: \n",
    "    - train a LLM from scratch\n",
    "    - large databasets required\n",
    "2. Model Fine Tuning:\n",
    "    - adapt a pre-trained LLM to spesific datasets/domains\n",
    "    - require thousands of domain-specific examples\n",
    "3. Passinfg contextual information\n",
    "   - combine LLM wtih domain knowleage retrieval\n",
    "   - require external domain knowleage base \n",
    "   - use vector seach and provide relevant context to LLM\n",
    "\n",
    "### RAG\n",
    "\n",
    "Why RAG/shortcomings of LLM \n",
    "- LLMs can only answer questions based on the <b>data they are trained on</b>.  \n",
    "- The answers from the LLMs may not be current. Their cut-off date is usually the date on which their original training data sources are extracted.\n",
    "- LLMs likely has <b>hallucination </b>. They sometimes provide make-believe answers that are not factually correct. \n",
    "- for enterprise use cases, LLMs cannot answer questions based on enterprise or confidential data where this data is not part of the training dataset. \n",
    "- It is possible to build custom LLMs using organizational data only, but that can prove to be expensive to build. It is also expensive to keep the LLM updated with new data on a daily basis. \n",
    "\n",
    "Benifts of RAG\n",
    "- up-to-data and accurate reponses\n",
    "- reduce inaccureate eresponses, mitigate the risk of producing hallucinations\n",
    "- domain-specific context\n",
    "- efficienct and cost-effective \n",
    "\n",
    "Passing context as model inputs improve factual recall\n",
    "Downsides of long contexts:\n",
    "- higher API cost (due to more token)\n",
    "- longer completion/inference time\n",
    "\n",
    "\n",
    "### Main concept of RAG\n",
    "1. Index and embedding: chrunk, embedding, index\n",
    "2. Vector store: knowleage DB\n",
    "3. Retieval: similarity search\n",
    "4. Filtering & Reranking\n",
    "5. Promopt augmentation \n",
    "6. Generation\n",
    "\n",
    "### RAG in Databricks\n",
    "<img src=\"documents/rag architectuer.jpeg\" alt=\"drawing\" style=\"width:600px;\"/>\n",
    "\n",
    "- Delta Live tables(batch/stream ingestion)\n",
    "- Mosaic AI model serving (LLM models, embedding models)\n",
    "- Mosaic AI Vector Search\n",
    "- Unity catalogs's Model registry\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
